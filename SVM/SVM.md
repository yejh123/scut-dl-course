# SVM

 - [SVM-支持向量机原理详解与实践之一](https://www.cnblogs.com/spoorer/archive/2004/01/13/6551220.html)
 - [SVM -支持向量机原理详解与实践之二](https://www.cnblogs.com/spoorer/archive/2004/01/13/6548727.html)
 - [SVM -支持向量机原理详解与实践之三](https://www.cnblogs.com/spoorer/archive/2004/01/13/6548818.html)
 - [SVM -支持向量机原理详解与实践之四](https://www.cnblogs.com/spoorer/archive/2004/01/13/6548885.html)
 - [SVM -支持向量机原理与实践之实践篇](https://www.cnblogs.com/spoorer/archive/2004/01/13/6649316.html)
 -  - [A Story of Basis and Kernel - Part I: Function Basis](http://songcy.net/posts/story-of-basis-and-kernel-part-1/)
 - [A Story of Basis and Kernel - Part II: Reproducing Kernel Hilbert Space](http://songcy.net/posts/story-of-basis-and-kernel-part-2/)

## 原理
给定训练样本，支持向量机建立一个超平面作为决策曲面，使得正例和反例的隔离边界最大化。

### 线性可分和线性不可分
线性可分-linearly separable, 在二维空间可以理解为可以用一条直线（一个函数）把两类型的样本隔开，被隔离开来的两类样本即为线性可分样本。同理在高维空间，可以理解为可以被一个曲面(高维函数)隔开的两类样本。

线性不可分，则可以理解为自变量和因变量之间的关系不是线性的。

实际上，线性可不分的情况更多，但是即使是非线性的样本通常也是通过高斯核函数将其映射到高维空间，在高维空间非线性的问题转化为线性可分的问题。


### 函数间隔和几何间隔
决策曲面方程如下：

![](http://img.blog.csdn.net/20170313212317680)

线性可分模式下最优超平面的示意图如下：

![](http://img.blog.csdn.net/20170313212320759)

 - p为分离边缘，即超平面和最近数据点的间隔。如果一个平面能使p最大，则为最优超平面。
 - 灰色的方形点和原形点就是我们所说的支持向量。


假设最优超平面的判别函数为：

![](http://img.blog.csdn.net/20170313212322322)

点x到最优超平面的距离:

![](http://img.blog.csdn.net/20170313212323260)

找到的这个最优超平面的参数w0和b0, 于是在样本向量集中, 有一对(w0,b0)一定满足（因为是常数，它只是决定了决策曲面相对原点的偏离）：

![](http://img.blog.csdn.net/20170313212331259)

满足上式的点就是支持向量，这些点距离决策曲面也就时超平面最近，是最难区分的点。

在超平面的正面和负面我们有任一支持向量满足代数距离：

![](http://img.blog.csdn.net/20170313212332352)

从上式可以得知，最大化两个类之间的分离边缘等价于最小化权值向量w的欧几里得范数。


### 支持向量
离决策平面最近的点就是支持向量，对应的拉格朗日乘子ai>0，其他点对应的拉格朗日乘子ai=0。

### 二次最优化
确定优化问题：

![](http://img.blog.csdn.net/20170313212336196)


我们发现以上两个式子都可以表示最大化间隔的优化问题，但是我们同时也发现无论上面哪个式子都是非凸的，并没有现成的可用的软件来解决这两种形式的优化问题。

于是一个行之有效的优化问题的形式被提出来，注意它是一个凸函数形式，如下：

![](http://img.blog.csdn.net/20170313212336744)

优化问题包含了一个凸二次优化对象并且线性可分，概括来说就是需找最优超平面的二次最优化，这个优化的问题可以用商业的凸二次规划代码来解。


### 拉格朗日对偶性（Largrange duality）深入分析

拉格朗日对偶性可以引导我们到优化问题的对偶形式，因为对偶形式在高维空间有效的运用核（函数）来得到最优间隔分类器的方法中扮演了非常重要的角色。对偶形式让我们得到一个有效的算法来解决上述的优化问题并且相较通用的二次规划商业软件更好。

优化问题的对偶形式的方法简单来说就是通过Lagrange Duality变换到对偶变量 (dual variable)的优化问题之后，应用拉格朗日对偶性，通过求解对偶问题得到最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：

 - 一是原问题的对偶问题往往更容易求解
 - 二者可以自然的引入核函数，进而推广到非线性分类问题。


所以以下可以将寻找最优超平面二次最优化（原问题），总结为以下几个步骤：

 - 在原始权重空间的带约束的优化问题。（注意带约束）
 - 对优化问题建立拉格朗日函数
 - 推导出机器的最优化条件
 - 最后就是在对偶空间解决带拉格朗日乘子的优化问题。

#### 对偶问题
如果原问题有最优解，对偶问题也有最优解，并且相应的最优值是相同的。


#### KTT条件
在某些条件下，把原始的约束问题通过拉格朗日函数转化为无约束问题，如果原始问题求解棘手，在满足KKT的条件下用求解对偶问题来代替求解原始问题，使得问题求解更加容易。


### 对优化问题建立拉格朗日函数
先固定拉格朗日乘子ai，优化出最优的w,b，最后再确定参数ai


### 核 (Kernel Trick)
将特征空间庞大运算量问题（或者说无法计算的问题）通过核技巧转化为低维空间可计算的问题。

 - 线性核(Linear) 主要应用与线性可分的情形，参数少，速度快，对于一般的数据可以尝试首先运用线性核。
 - RBF也就是径向基函数，也叫高斯核，应用最为广泛，主要应用线性不可分的情形，参数多，分类结果非常依赖于参数。通过交叉验证来寻找合适的参数，通过大量的训练可以达到比线性核更好的效果。
 - 多项式核需要确定的参数要比RBF多，而参数多少直接影响了模型的复杂度。
 - Sigmoid核，对于某些参数RBF和sigmoid具有相似的性能。


#### 核函数判定与再生核希尔伯特空间(reproducing kernel Hilbert space, RKHS)
Mercer定理：

如果函数K是 R^n * R^n -> R 上的映射(也就是从两个n维向量映射到实数域)。那么如果K是一个有效的核函数，也称为Mercer核函数，那么当且仅当对于训练样例{x1,x2,...,xm}, 其相应的核函数矩阵是半正定的。


只要一个对称函数所对应的核矩阵半正定就可以作为核函数使用,任何一个核函数都隐式地定义一个称为"再生核希尔伯特空间"（reproducing kernel Hilbert space, RKHS）的特征空间。


### 软间隔和正则化

#### 离群样本点和损失函数分析
前面介绍的模型对这些离群的点或者说是样本的噪声非常敏感。

这里要提到的软间隔就是，就是支持向量机允许一些样本出错，即允许存在一些不满足约束的离群的点，如下图红色高显的点，就是一些不满足约束的离群点。


#### 替代损失(surrogate loss)简介与松弛变量(Slack variables)
替代损失，顾名思义，就是为了优化有样本离群这种情况的目标函数而引入的损失函数。

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314153632557-706320531.png)

这就是常用的"软间隔支持向量机"。

 - 函数间隔小于1的样本点在最大间隔区间里面，如下图中的蓝圈圈住的样本。
 - 函数间隔为负数的样本点则在在相反的样本分类里面，如下图中的红圈圈住的样本。

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314153635854-2068993382.png)


#### 拉格朗日求解软间隔目标函数
软间隔支持向量机优化问题定义：

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314153645385-427652087.png)

通过目标函数和约束条件构造拉格朗日函数：

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314153640323-833438026.png)

我们的对偶问题，就是通过固定拉格朗日乘子a，得到w和b的最优化表达式（关于a的表达式），所以最后我们只需要确认a，我们就可以最终确定w和b。

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314154553557-1359694260.png)


### SMO算法

SMO即Sequential minmal optimization, 是最快的二次规划的优化算法，特使对线性SVM和稀疏数据性能更优。在正式介绍SMO算法之前，首先要了解坐标上升法。

#### 坐标上升法(Coordinate ascent)



#### SMO算法在SVM的应用
带求解问题：

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314154553557-1359694260.png)

重复大括号中的操作直到收敛{

 - 选择一对和来更新下一个(用启发式的方法，也就是尝试选取两个允许我们朝着全局最大方向做最大前进的参数)。

 - 固定所有其它的参数，优化关于和的函数W(a)。

}


![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314154611901-2105798578.png)


由于右边固定，我们可以直接用一个常数表示，例如用？表示：

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314154613401-1697672334.png)

于是我可以将a1和a2的约束画出来：

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314154614995-379711447.png)

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314154620698-1098949210.png)

上式第三行有误。

其中y1*y1=1，因为y1={-1,1}，所以有：

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314154623666-1679268834.png)

所以目标问题W可以表示为：

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314154623979-1728008222.png)

实际的问题中W展开后就是一个关于的二次函数，这样通过对W进行求导可得a2，然而要保证a2满足L<=a2<=H：

![](https://images2015.cnblogs.com/blog/1125535/201703/1125535-20170314154628041-1006434697.png)


a2求出来后，就可以得到a1。























